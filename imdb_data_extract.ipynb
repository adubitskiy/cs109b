{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_part(file_name):\n",
    "    with open(file_name, 'rb') as handle:\n",
    "        return cPickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from the IMDB movies. Since we cannot have the in a single pickle file this code goes over the parts and gets the required columns.\n",
    "The parts is here https://drive.google.com/file/d/0B9PSivXSSQOTckNpR0RWcW14NkU/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# location of the IMDB parts\n",
    "imdb_parts_folder = 'imdb_parts/'\n",
    "# if you want the results to be pickled\n",
    "result_file_name = 'imdb_cast_director.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coulmnes to copy from the IMDB movie objects\n",
    "# the resultig object is a dictionary with the two keys at most\n",
    "columns = ['cast', 'director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add required tmdb movie ids to the set to filter \n",
    "# if empty - no filtering but the result might still be too big to fit in memory\n",
    "required_ids = set([618])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb_info.pickle0',\n",
       " 'imdb_info.pickle1',\n",
       " 'imdb_info.pickle10',\n",
       " 'imdb_info.pickle11',\n",
       " 'imdb_info.pickle12',\n",
       " 'imdb_info.pickle13',\n",
       " 'imdb_info.pickle14',\n",
       " 'imdb_info.pickle15',\n",
       " 'imdb_info.pickle16',\n",
       " 'imdb_info.pickle17',\n",
       " 'imdb_info.pickle18',\n",
       " 'imdb_info.pickle19',\n",
       " 'imdb_info.pickle2',\n",
       " 'imdb_info.pickle20',\n",
       " 'imdb_info.pickle21',\n",
       " 'imdb_info.pickle22',\n",
       " 'imdb_info.pickle23',\n",
       " 'imdb_info.pickle24',\n",
       " 'imdb_info.pickle25',\n",
       " 'imdb_info.pickle3',\n",
       " 'imdb_info.pickle4',\n",
       " 'imdb_info.pickle5',\n",
       " 'imdb_info.pickle6',\n",
       " 'imdb_info.pickle7',\n",
       " 'imdb_info.pickle8',\n",
       " 'imdb_info.pickle9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_files = [f for f in listdir(imdb_parts_folder) if isfile(join(imdb_parts_folder, f))]\n",
    "part_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  imdb_info.pickle0\n",
      "Processing  imdb_info.pickle1\n",
      "Processing  imdb_info.pickle10\n",
      "Processing  imdb_info.pickle11\n",
      "Processing  imdb_info.pickle12\n",
      "Processing  imdb_info.pickle13\n",
      "Processing  imdb_info.pickle14\n",
      "Processing  imdb_info.pickle15\n",
      "Processing  imdb_info.pickle16\n",
      "Processing  imdb_info.pickle17\n",
      "Processing  imdb_info.pickle18\n",
      "Processing  imdb_info.pickle19\n",
      "Processing  imdb_info.pickle2\n",
      "Processing  imdb_info.pickle20\n",
      "Processing  imdb_info.pickle21\n",
      "Processing  imdb_info.pickle22\n",
      "Processing  imdb_info.pickle23\n",
      "Processing  imdb_info.pickle24\n",
      "Processing  imdb_info.pickle25\n",
      "Processing  imdb_info.pickle3\n",
      "Processing  imdb_info.pickle4\n",
      "Processing  imdb_info.pickle5\n",
      "Processing  imdb_info.pickle6\n",
      "Processing  imdb_info.pickle7\n",
      "Processing  imdb_info.pickle8\n",
      "Processing  imdb_info.pickle9\n",
      "Total created:48824\n"
     ]
    }
   ],
   "source": [
    "imdb_dict = {}\n",
    "required_defined = len(required_ids) > 0\n",
    "# iterate all files\n",
    "for fname in part_files:\n",
    "    # we collecte all we need\n",
    "    if(required_defined and len(required_ids.difference(imdb_dict.keys())) == 0):\n",
    "        break\n",
    "    print 'Processing ', fname\n",
    "    # load the part\n",
    "    part_dict = load_part(join(imdb_parts_folder, fname))\n",
    "    # for each movie in the part\n",
    "    for tmdb_id, imdb_movie in part_dict.iteritems():\n",
    "        if(required_defined and tmdb_id not in required_ids):\n",
    "            # we do not need this movie\n",
    "            continue\n",
    "        small_imdb_movie = {}\n",
    "        # try copy required columns\n",
    "        for column in columns:\n",
    "            # check if the column exists\n",
    "            if(column in imdb_movie.keys()):\n",
    "                small_imdb_movie[column] = imdb_movie[column]\n",
    "        # check if at least one of the columns copied\n",
    "        if(len(small_imdb_movie) > 0):\n",
    "            imdb_dict[tmdb_id] = small_imdb_movie\n",
    "        \n",
    "print 'Total created:' + str(len(imdb_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result file is dumped into the parts folder, don't forget to remove it before running this again or it will be picked up for processing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if result_file_name is not None:\n",
    "    with open(join(imdb_parts_folder, result_file_name), 'wb') as handle:\n",
    "        cPickle.dump(imdb_dict, handle, protocol = cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random result:\n",
      "{'director': [<Person id:0240995[http] name:_Duguay, Christian_>], 'cast': [<Person id:0011164[http] name:_Adams, Lynne_>, <Person id:4499016[http] name:_Aggeliki, Zoe_>, <Person id:0086396[http] name:_Blais, Isabelle_>, <Person id:0095053[http] name:_Bonnier, Céline_>, <Person id:0100333[http] name:_Boutin, David_>, <Person id:0132441[http] name:_Campbell, Emma_>, <Person id:0001015[http] name:_Carlyle, Robert_>, <Person id:0285603[http] name:_Ford, Dawn_>, <Person id:0320721[http] name:_Girard, Rémy_>, <Person id:1965525[http] name:_Hopkins, Anna_>, <Person id:0472761[http] name:_Krupa, Mark Antony_>, <Person id:1436204[http] name:_Labrosse, Sarah-Jeanne_>, <Person id:0495799[http] name:_Leboeuf, Laurence_>, <Person id:6272880[http] name:_Nikolavcic, Davorin_>, <Person id:0815257[http] name:_Sorvino, Michael_>, <Person id:0000227[http] name:_Sorvino, Mira_>, <Person id:0000661[http] name:_Sutherland, Donald_>, <Person id:0903985[http] name:_Vrana, Vlasta_>]}\n"
     ]
    }
   ],
   "source": [
    "print 'Random result:'\n",
    "print imdb_dict[random.choice(imdb_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
